{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91547296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.dataset.common import ListDataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from My_DeepAREstimator import my_DeepAREstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec0ed9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create train / dev sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856239c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " #Create train / dev sample\n",
    "def creat_dict_data( df_input, data_type, data_config):\n",
    "    \n",
    "    target_var = data_config['target_var']\n",
    "    covariate = data_config['covariate']\n",
    "    start_time = data_config[data_type + '_period']['start']\n",
    "    end_time = data_config[data_type + '_period']['end']\n",
    "    \n",
    "    Data = []\n",
    "    for s, e in zip(start_time, end_time):\n",
    "        for idx, var in enumerate(target_var):\n",
    "            d = {\"start\": s, \"target\": df_input.loc[s:e][var]}\n",
    "            if len(target_var) > 1:\n",
    "                d['feat_static_cat'] = [idx]\n",
    "            if len(covariate) != 0:\n",
    "                d['feat_dynamic_real'] = df_input.loc[s:e, covariate].T.values\n",
    "            Data.append(d)\n",
    "\n",
    "    if data_config['show_train_valid_dict_data']:\n",
    "        print(f'create {len(Data)} dict_data, The first sampe  dict_data:')\n",
    "        print(Data[0])\n",
    "        # for idx, d in enumerate(Data):\n",
    "        #     print(f'The {idx} dict_data:\\n', d)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a1e4a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DeepAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6191fd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DeepAR Model\n",
    "def DeepAR_model(dataset, data_config, model_config):\n",
    "\n",
    "    freq = data_config['freq']\n",
    "    target_var = data_config['target_var']\n",
    "    covariate = data_config['covariate']\n",
    "    \n",
    "    print()\n",
    "    print(\"=\"*40)\n",
    "    print('Create Training Set')\n",
    "    training_data = ListDataset(creat_dict_data(dataset[::freq], \n",
    "                                                'train', data_config),\n",
    "                                freq=str(freq) + \"min\")\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print('Create Development Set')\n",
    "    dev_data = ListDataset(creat_dict_data(dataset[::freq],\n",
    "                                                'dev', data_config),\n",
    "                                freq=str(freq) + \"min\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # build model\n",
    "    print('Build and Train the DeepAR model')\n",
    "    estimator = my_DeepAREstimator(\n",
    "        freq=str(freq) + \"min\",\n",
    "        context_length = data_config['context_length'],\n",
    "        prediction_length = data_config['prediction_length'],\n",
    "        use_feat_dynamic_real = True if len(covariate) != 0 else False,\n",
    "        use_feat_static_cat = True if len(target_var) > 1 else False,\n",
    "        cardinality = [len(target_var)] if len(target_var) > 1 else None,\n",
    "        num_layers = model_config['num_layers'],\n",
    "        num_cells = model_config['num_cells'],\n",
    "        cell_type = model_config['cell_type'],\n",
    "        trainer=Trainer(epochs=model_config['epochs'], \n",
    "                        ctx=model_config['ctx']))\n",
    "    \n",
    "    # Train Model\n",
    "    mx.random.seed(model_config['r_seed'])\n",
    "    predictor = estimator.train(training_data = training_data,\n",
    "                                validation_data = dev_data,\n",
    "                                continue_train = model_config['continue_train'],\n",
    "                                model_path = model_config['continue_train_model'])\n",
    "    \n",
    "    \n",
    "    # Save Model\n",
    "    model_name = model_config['model_name']\n",
    "    if not os.path.exists('DeepAR_Model'):\n",
    "        os.mkdir('DeepAR_Model')\n",
    "    if model_config['save_model']:\n",
    "        print('Saving the model ...')\n",
    "        if os.path.exists(f'DeepAR_Model/{model_name}'):\n",
    "             # delete the old model\n",
    "            shutil.rmtree(f'DeepAR_Model/{model_name}') \n",
    "        os.mkdir(f'DeepAR_Model/{model_name}')\n",
    "        predictor.serialize(Path(f'DeepAR_Model/{model_name}'))\n",
    "\n",
    "    return (predictor, training_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da46f52",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f04b3ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_model(data_config, model_config):\n",
    "    # Format data\n",
    "    PATH_1 = 'clean data'\n",
    "    Features_Path = 'new features.csv'\n",
    "    \n",
    "    target_var = data_config['target_var']\n",
    "    \n",
    "    # Concate water consumption data into one dataframe df_wc\n",
    "    df_wc_0 = pd.read_csv(f'{PATH_1}\\\\{target_var[0]}.csv', index_col=0)\n",
    "    df_wc = df_wc_0.copy()\n",
    "    for station in target_var[1:]:\n",
    "        df_wc_i = pd.read_csv(f'{PATH_1}\\\\{station}.csv', index_col=0)\n",
    "        df_wc = np.hstack((np.array(df_wc), np.array(df_wc_i)))\n",
    "\n",
    "    # data_config['target_var']= [station + '_Water_Consumption' for station in target_var]\n",
    "    df_wc = pd.DataFrame(df_wc, index=df_wc_0.index, columns=target_var)\n",
    "    \n",
    "    # get other features into df_features\n",
    "    df_features = pd.read_csv(Features_Path, index_col=0)\n",
    "    \n",
    "    # Concate water consumption series and other features into one dataframe\n",
    "    df = pd.DataFrame(np.hstack((np.array(df_wc), np.array(df_features))),\n",
    "                      index=df_wc.index,\n",
    "                      columns=np.append(df_wc.columns.values,\n",
    "                                        df_features.columns.values))\n",
    "    # Normalize df\n",
    "    df_01 = (df - df.min()) / (df.max() - df.min())\n",
    "    \n",
    "    # Built and train DeepAR model\n",
    "    (predictor, training_data, dev_data) = DeepAR_model(df_01, data_config, model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a7ba0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# get_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45293e73",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_model_name(data_config, model_config):\n",
    "    \n",
    "    target_var = data_config['target_var']\n",
    "    covariate = data_config['covariate']\n",
    "    freq = data_config['freq']\n",
    "    prediction_length = data_config['prediction_length']\n",
    "    context_length = data_config['context_length']\n",
    "    cell_type = model_config['cell_type']\n",
    "    # continue_train = str(model_config['continue_train'] + 0)\n",
    "    \n",
    "    model_name = '_'.join(target_var) + '_' + '_'.join(covariate)  \\\n",
    "        + '_' + '_'.join((str(freq), str(context_length), str(prediction_length), cell_type))\n",
    "        \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ada19f",
   "metadata": {},
   "source": [
    "# Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260f3b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Create Training Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2019-01-01 00:00', 'target': 2019-01-01 00:00    0.404908\n",
      "2019-01-01 00:10    0.420715\n",
      "2019-01-01 00:20    0.289434\n",
      "2019-01-01 00:30    0.336855\n",
      "2019-01-01 00:40    0.309983\n",
      "                      ...   \n",
      "2020-08-06 23:20    0.320715\n",
      "2020-08-06 23:30    0.377621\n",
      "2020-08-06 23:40    0.354077\n",
      "2020-08-06 23:50    0.343927\n",
      "2020-08-07 00:00    0.329700\n",
      "Name: HY, Length: 84097, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.465252  , 0.465252  , 0.465252  , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.18918919, 0.18918919, 0.18918919, ..., 0.91891892, 0.91891892,\n",
      "        0.94594595]])}\n",
      "========================================\n",
      "Create Development Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2020-08-07 00:00', 'target': 2020-08-07 00:00    0.329700\n",
      "2020-08-07 00:10    0.306240\n",
      "2020-08-07 00:20    0.269384\n",
      "2020-08-07 00:30    0.259817\n",
      "2020-08-07 00:40    0.228369\n",
      "                      ...   \n",
      "2020-10-18 23:20    0.341930\n",
      "2020-10-18 23:30    0.324210\n",
      "2020-10-18 23:40    0.308569\n",
      "2020-10-18 23:50    0.312895\n",
      "2020-10-19 00:00    0.281364\n",
      "Name: HY, Length: 10513, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        3.34039481e-05, 3.34039481e-05, 1.22886258e-05],\n",
      "       [9.45945946e-01, 9.45945946e-01, 9.45945946e-01, ...,\n",
      "        5.67567568e-01, 5.67567568e-01, 5.67567568e-01]])}\n",
      "========================================\n",
      "Build and Train the DeepAR model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [00:51<00:00,  1.03s/it, epoch=1/20, avg_epoch_loss=-1.09]\n",
      "1it [00:02,  2.46s/it, epoch=1/20, validation_avg_epoch_loss=-1.78]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.02it/s, epoch=2/20, avg_epoch_loss=-1.82]\n",
      "1it [00:02,  2.46s/it, epoch=2/20, validation_avg_epoch_loss=-2]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=3/20, avg_epoch_loss=-1.94]\n",
      "1it [00:02,  2.50s/it, epoch=3/20, validation_avg_epoch_loss=-2.04]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.02it/s, epoch=4/20, avg_epoch_loss=-2.07]\n",
      "1it [00:02,  2.57s/it, epoch=4/20, validation_avg_epoch_loss=-1.95]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:48<00:00,  1.02it/s, epoch=5/20, avg_epoch_loss=-2.08]\n",
      "1it [00:02,  2.58s/it, epoch=5/20, validation_avg_epoch_loss=-2.19]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.02it/s, epoch=6/20, avg_epoch_loss=-2.15]\n",
      "1it [00:02,  2.57s/it, epoch=6/20, validation_avg_epoch_loss=-2.21]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=7/20, avg_epoch_loss=-2.17]\n",
      "1it [00:02,  2.67s/it, epoch=7/20, validation_avg_epoch_loss=-2.22]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=8/20, avg_epoch_loss=-2.19]\n",
      "1it [00:02,  2.79s/it, epoch=8/20, validation_avg_epoch_loss=-2.14]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.00it/s, epoch=9/20, avg_epoch_loss=-2.21]\n",
      "1it [00:02,  2.80s/it, epoch=9/20, validation_avg_epoch_loss=-2.14]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=10/20, avg_epoch_loss=-2.22]\n",
      "1it [00:02,  2.86s/it, epoch=10/20, validation_avg_epoch_loss=-2.28]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.00s/it, epoch=11/20, avg_epoch_loss=-2.2]\n",
      "1it [00:02,  2.81s/it, epoch=11/20, validation_avg_epoch_loss=-2.15]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=12/20, avg_epoch_loss=-2.27]\n",
      "1it [00:03,  3.09s/it, epoch=12/20, validation_avg_epoch_loss=-2.1]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=13/20, avg_epoch_loss=-2.25]\n",
      "1it [00:03,  3.01s/it, epoch=13/20, validation_avg_epoch_loss=-2.34]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=14/20, avg_epoch_loss=-2.26]\n",
      "1it [00:03,  3.11s/it, epoch=14/20, validation_avg_epoch_loss=-2.29]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=15/20, avg_epoch_loss=-2.28]\n",
      "1it [00:03,  3.12s/it, epoch=15/20, validation_avg_epoch_loss=-2.29]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.02s/it, epoch=16/20, avg_epoch_loss=-2.33]\n",
      "1it [00:03,  3.11s/it, epoch=16/20, validation_avg_epoch_loss=-2.35]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.02s/it, epoch=17/20, avg_epoch_loss=-2.29]\n",
      "1it [00:03,  3.06s/it, epoch=17/20, validation_avg_epoch_loss=-2.37]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:51<00:00,  1.02s/it, epoch=18/20, avg_epoch_loss=-2.3]\n",
      "1it [00:03,  3.08s/it, epoch=18/20, validation_avg_epoch_loss=-2.37]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=19/20, avg_epoch_loss=-2.3]\n",
      "1it [00:03,  3.19s/it, epoch=19/20, validation_avg_epoch_loss=-2.15]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=20/20, avg_epoch_loss=-2.31]\n",
      "1it [00:03,  3.17s/it, epoch=20/20, validation_avg_epoch_loss=-2.29]\n",
      "WARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_config = dict(target_var = ['HY', 'WJ'],\n",
    "                       freq = 10,\n",
    "                       prediction_length = 144,\n",
    "                       context_length = 720,\n",
    "                       covariate = ['HF_e1','MT'],\n",
    "                       show_train_valid_dict_data = True,\n",
    "                       )\n",
    "    model_config = dict(r_seed = 0,\n",
    "                        num_layers = 2,\n",
    "                        num_cells = 50,\n",
    "                        cell_type = 'lstm',\n",
    "                        epochs = 20,\n",
    "                        ctx = 'gpu',\n",
    "                        save_model = True)\n",
    "# =============================================================================\n",
    "# Basic model\n",
    "# =============================================================================\n",
    "\n",
    "    data_config['train_period'] = {\n",
    "        'start': ['2019-01-01 00:00'],\n",
    "        'end': ['2020-08-07 00:00']}\n",
    "    \n",
    "    data_config['dev_period'] = {\n",
    "        'start': ['2020-08-07 00:00'],\n",
    "        'end': ['2020-10-19 00:00']\n",
    "    }\n",
    "    \n",
    "    model_config['continue_train'] = False\n",
    "    model_config['continue_train_model'] = None\n",
    "    model_config['model_name'] = get_model_name(data_config, model_config)\n",
    "    run_model(data_config, model_config)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba716513",
   "metadata": {},
   "source": [
    "# Real-time model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da005da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Create Training Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2019-01-01 00:00', 'target': 2019-01-01 00:00    0.404908\n",
      "2019-01-01 00:10    0.420715\n",
      "2019-01-01 00:20    0.289434\n",
      "2019-01-01 00:30    0.336855\n",
      "2019-01-01 00:40    0.309983\n",
      "                      ...   \n",
      "2019-10-06 23:20    0.378785\n",
      "2019-10-06 23:30    0.352246\n",
      "2019-10-06 23:40    0.349917\n",
      "2019-10-06 23:50    0.338103\n",
      "2019-10-07 00:00    0.371464\n",
      "Name: HY, Length: 40177, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.465252  , 0.465252  , 0.465252  , ..., 0.90535007, 0.90535007,\n",
      "        0.73534667]])}\n",
      "========================================\n",
      "Create Development Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2019-10-07 00:00', 'target': 2019-10-07 00:00    0.371464\n",
      "2019-10-07 00:10    0.342013\n",
      "2019-10-07 00:20    0.325374\n",
      "2019-10-07 00:30    0.323710\n",
      "2019-10-07 00:40    0.316389\n",
      "                      ...   \n",
      "2019-11-18 23:20    0.343760\n",
      "2019-11-18 23:30    0.360566\n",
      "2019-11-18 23:40    0.338769\n",
      "2019-11-18 23:50    0.343677\n",
      "2019-11-19 00:00    0.282030\n",
      "Name: HY, Length: 6193, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.73534667, 0.73534667, 0.73534667, ..., 0.        , 0.        ,\n",
      "        0.        ]])}\n",
      "========================================\n",
      "Build and Train the DeepAR model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [00:54<00:00,  1.09s/it, epoch=1/10, avg_epoch_loss=-1.04]\n",
      "1it [00:03,  3.33s/it, epoch=1/10, validation_avg_epoch_loss=-1.71]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=2/10, avg_epoch_loss=-1.71]\n",
      "1it [00:03,  3.21s/it, epoch=2/10, validation_avg_epoch_loss=-1.79]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=3/10, avg_epoch_loss=-1.84]\n",
      "1it [00:03,  3.18s/it, epoch=3/10, validation_avg_epoch_loss=-1.58]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=4/10, avg_epoch_loss=-1.92]\n",
      "1it [00:03,  3.14s/it, epoch=4/10, validation_avg_epoch_loss=-2.07]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=5/10, avg_epoch_loss=-1.97]\n",
      "1it [00:03,  3.17s/it, epoch=5/10, validation_avg_epoch_loss=-2.02]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=6/10, avg_epoch_loss=-1.99]\n",
      "1it [00:03,  3.15s/it, epoch=6/10, validation_avg_epoch_loss=-2.12]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.01it/s, epoch=7/10, avg_epoch_loss=-2.05]\n",
      "1it [00:03,  3.14s/it, epoch=7/10, validation_avg_epoch_loss=-2.09]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=8/10, avg_epoch_loss=-2.03]\n",
      "1it [00:03,  3.16s/it, epoch=8/10, validation_avg_epoch_loss=-2.12]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.00it/s, epoch=9/10, avg_epoch_loss=-2.08]\n",
      "1it [00:03,  3.23s/it, epoch=9/10, validation_avg_epoch_loss=-2.14]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:49<00:00,  1.00it/s, epoch=10/10, avg_epoch_loss=-2.07]\n",
      "1it [00:03,  3.18s/it, epoch=10/10, validation_avg_epoch_loss=-2.1]\n",
      "WARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_mu_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_mu_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_sigma_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_sigma_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_nu_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_None_distr_nu_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm0_i2h_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm0_h2h_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm0_i2h_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm0_h2h_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm1_i2h_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm1_h2h_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm1_i2h_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_lstm1_h2h_bias' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "c:\\mxnet\\ci\\windows_package\\python\\mxnet\\gluon\\parameter.py:896: UserWarning: Parameter 'deepartrainingnetwork5_featureembedder0_cat_0_embedding_weight' is already initialized, ignoring. Set force_reinit=True to re-initialize.\n",
      "  v.initialize(None, ctx, init, force_reinit=force_reinit)\n",
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Create Training Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2019-10-07 00:00', 'target': 2019-10-07 00:00    0.371464\n",
      "2019-10-07 00:10    0.342013\n",
      "2019-10-07 00:20    0.325374\n",
      "2019-10-07 00:30    0.323710\n",
      "2019-10-07 00:40    0.316389\n",
      "                      ...   \n",
      "2020-08-06 23:20    0.320715\n",
      "2020-08-06 23:30    0.377621\n",
      "2020-08-06 23:40    0.354077\n",
      "2020-08-06 23:50    0.343927\n",
      "2020-08-07 00:00    0.329700\n",
      "Name: HY, Length: 43921, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.73534667, 0.73534667, 0.73534667, ..., 0.        , 0.        ,\n",
      "        0.        ]])}\n",
      "========================================\n",
      "Create Development Set\n",
      "create 2 dict_data, The first sampe  dict_data:\n",
      "{'start': '2020-08-07 00:00', 'target': 2020-08-07 00:00    0.329700\n",
      "2020-08-07 00:10    0.306240\n",
      "2020-08-07 00:20    0.269384\n",
      "2020-08-07 00:30    0.259817\n",
      "2020-08-07 00:40    0.228369\n",
      "                      ...   \n",
      "2020-10-18 23:20    0.341930\n",
      "2020-10-18 23:30    0.324210\n",
      "2020-10-18 23:40    0.308569\n",
      "2020-10-18 23:50    0.312895\n",
      "2020-10-19 00:00    0.281364\n",
      "Name: HY, Length: 10513, dtype: float64, 'feat_static_cat': [0], 'feat_dynamic_real': array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "        3.34039481e-05, 3.34039481e-05, 1.22886258e-05]])}\n",
      "========================================\n",
      "Build and Train the DeepAR model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 50/50 [00:53<00:00,  1.07s/it, epoch=1/10, avg_epoch_loss=-1.79]\n",
      "1it [00:03,  3.43s/it, epoch=1/10, validation_avg_epoch_loss=-2.15]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=2/10, avg_epoch_loss=-2.31]\n",
      "1it [00:03,  3.37s/it, epoch=2/10, validation_avg_epoch_loss=-2.27]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=3/10, avg_epoch_loss=-2.35]\n",
      "1it [00:03,  3.50s/it, epoch=3/10, validation_avg_epoch_loss=-2.28]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=4/10, avg_epoch_loss=-2.36]\n",
      "1it [00:03,  3.36s/it, epoch=4/10, validation_avg_epoch_loss=-2.25]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.02s/it, epoch=5/10, avg_epoch_loss=-2.36]\n",
      "1it [00:03,  3.31s/it, epoch=5/10, validation_avg_epoch_loss=-2.29]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=6/10, avg_epoch_loss=-2.41]\n",
      "1it [00:03,  3.42s/it, epoch=6/10, validation_avg_epoch_loss=-2.34]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=7/10, avg_epoch_loss=-2.43]\n",
      "1it [00:03,  3.29s/it, epoch=7/10, validation_avg_epoch_loss=-2.21]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.00s/it, epoch=8/10, avg_epoch_loss=-2.42]\n",
      "1it [00:03,  3.32s/it, epoch=8/10, validation_avg_epoch_loss=-2.34]\n",
      "100%|████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.01s/it, epoch=9/10, avg_epoch_loss=-2.44]\n",
      "1it [00:03,  3.42s/it, epoch=9/10, validation_avg_epoch_loss=-2.25]\n",
      "100%|███████████████████████████████████████████████| 50/50 [00:49<00:00,  1.00it/s, epoch=10/10, avg_epoch_loss=-2.43]\n",
      "1it [00:03,  3.44s/it, epoch=10/10, validation_avg_epoch_loss=-2.35]\n",
      "WARNING:root:Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_config = dict(target_var = ['HY', 'WJ'],\n",
    "                       freq = 10,\n",
    "                       prediction_length = 144,\n",
    "                       context_length = 720,\n",
    "                       covariate = ['HF_e1'],\n",
    "                       show_train_valid_dict_data = True,\n",
    "                       )\n",
    "    \n",
    "    model_config = dict(r_seed = 0,\n",
    "                        num_layers = 2,\n",
    "                        num_cells = 50,\n",
    "                        cell_type = 'lstm',\n",
    "                        epochs = 10,\n",
    "                        ctx = 'gpu',\n",
    "                        save_model = True)\n",
    "    \n",
    "# =============================================================================\n",
    "# Updata model real time\n",
    "# =============================================================================\n",
    "\n",
    "    # raw model \n",
    "    data_config['train_period']  = {\n",
    "        'start': ['2019-01-01 00:00'],\n",
    "        'end': ['2019-10-07 00:00']}\n",
    "    data_config['dev_period'] = {\n",
    "        'start': ['2019-10-07 00:00'],\n",
    "        'end': ['2019-11-19 00:00']\n",
    "    }\n",
    "    model_config['continue_train'] = False\n",
    "    model_config['continue_train_model'] = None\n",
    "    model_config['model_name'] = get_model_name(data_config, model_config) + '_raw'\n",
    "    run_model(data_config, model_config)\n",
    "    \n",
    "    # update\n",
    "    data_config['train_period'] = {\n",
    "        'start': ['2019-10-07 00:00'],\n",
    "        'end': ['2020-08-07 00:00']}\n",
    "    data_config['dev_period'] = {\n",
    "        'start': ['2020-08-07 00:00'],\n",
    "        'end': ['2020-10-19 00:00']\n",
    "    }\n",
    "    model_config['continue_train'] = True\n",
    "    model_config['continue_train_model'] = 'DeepAR_Model/' + get_model_name(data_config, model_config) + '_raw'\n",
    "    model_config['model_name'] = get_model_name(data_config, model_config) + '_update'\n",
    "    run_model(data_config, model_config)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
